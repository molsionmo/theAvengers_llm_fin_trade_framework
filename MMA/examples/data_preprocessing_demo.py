#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†ç¤ºä¾‹è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®é¢„å¤„ç†åŠŸèƒ½
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.preprocessor import DataPreprocessor
from src.data.dataset_loader import DatasetLoader
from src.data.task_dataset import TaskAwareDataset


def demo_dataset_preprocessing():
    """æ¼”ç¤ºæ•°æ®é›†é¢„å¤„ç†"""
    print("ğŸ¯ æ•°æ®é¢„å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # 1. åˆ›å»ºé¢„å¤„ç†å™¨
    print("\n1ï¸âƒ£ åˆ›å»ºæ•°æ®é¢„å¤„ç†å™¨...")
    preprocessor = DataPreprocessor(cache_dir="./demo_cache")
    
    # 2. åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    print("\n2ï¸âƒ£ å¯ç”¨æ•°æ®é›†:")
    datasets = preprocessor.list_available_datasets()
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„æ˜¾ç¤º
    task_groups = {}
    for name, config in datasets.items():
        task_type = config['task_type']
        if task_type not in task_groups:
            task_groups[task_type] = []
        task_groups[task_type].append(name)
    
    for task_type, dataset_names in task_groups.items():
        print(f"\n   ğŸ“‹ {task_type.replace('_', ' ').title()}:")
        for dataset_name in sorted(dataset_names)[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"     â€¢ {dataset_name}")
    
    # 3. é€‰æ‹©å°‘é‡æ•°æ®é›†è¿›è¡Œæ¼”ç¤º
    demo_datasets = ['imdb', 'ag_news']  # é€‰æ‹©ä¸¤ä¸ªè¾ƒå°çš„æ•°æ®é›†
    print(f"\n3ï¸âƒ£ é€‰æ‹©æ¼”ç¤ºæ•°æ®é›†: {demo_datasets}")
    
    try:
        # 4. åˆ›å»ºæ··åˆæ•°æ®é›†
        print("\n4ï¸âƒ£ åˆ›å»ºæ··åˆæ•°æ®é›†...")
        mixed_data = preprocessor.create_mixed_dataset(
            dataset_names=demo_datasets,
            max_samples_per_dataset=50,  # æ¯ä¸ªæ•°æ®é›†åªå–50ä¸ªæ ·æœ¬ç”¨äºæ¼”ç¤º
            split='train'
        )
        
        print(f"   âœ… ä»»åŠ¡æ£€æµ‹æ•°æ®: {len(mixed_data['task_detection_data'])} æ¡è®°å½•")
        print(f"   âœ… é€‚é…å™¨è®­ç»ƒæ•°æ®: {len(mixed_data['adapter_training_data']['input_texts'])} æ¡è®°å½•")
        
        # 5. ä¿å­˜æ•°æ®
        print("\n5ï¸âƒ£ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        output_dir = "./demo_processed_data"
        preprocessor.save_processed_data(mixed_data, output_dir)
        print(f"   ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
        
        # 6. åŠ è½½å’ŒéªŒè¯æ•°æ®
        print("\n6ï¸âƒ£ åŠ è½½å’ŒéªŒè¯æ•°æ®...")
        loader = DatasetLoader(output_dir)
        
        # è·å–ä»»åŠ¡æ£€æµ‹æ•°æ®é›†
        task_dataset = loader.get_task_detection_dataset()
        print(f"   ğŸ” ä»»åŠ¡æ£€æµ‹æ•°æ®é›†: {len(task_dataset)} æ¡è®°å½•")
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample = task_dataset[0]
        print(f"   ğŸ“ ç¤ºä¾‹æ ·æœ¬:")
        print(f"      ä»»åŠ¡ç±»å‹: {sample['task_type']}")
        print(f"      æ–‡æœ¬: {sample['text'][:100]}...")
        
        # è·å–é€‚é…å™¨è®­ç»ƒæ•°æ®é›†
        adapter_dataset = loader.get_adapter_training_dataset()
        print(f"   ğŸ”§ é€‚é…å™¨è®­ç»ƒæ•°æ®é›†: {len(adapter_dataset)} æ¡è®°å½•")
        
        # 7. åˆ›å»ºä»»åŠ¡æ„ŸçŸ¥æ•°æ®é›†
        print("\n7ï¸âƒ£ åˆ›å»ºä»»åŠ¡æ„ŸçŸ¥æ•°æ®é›†...")
        task_aware_dataset = TaskAwareDataset(
            adapter_dataset.data,
            adapter_dataset.tokenizer,
            max_length=128,  # çŸ­åºåˆ—ç”¨äºæ¼”ç¤º
            task_sampling_strategy='balanced',
            include_task_tokens=True
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = task_aware_dataset.get_task_statistics()
        print(f"   ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"      æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"      ä»»åŠ¡ç±»å‹æ•°: {stats['num_tasks']}")
        print(f"      å¹³å‡è¾“å…¥é•¿åº¦: {stats['avg_input_length']:.1f} è¯")
        print(f"      å¹³å‡ç›®æ ‡é•¿åº¦: {stats['avg_target_length']:.1f} è¯")
        
        print(f"   ğŸ“ˆ ä»»åŠ¡åˆ†å¸ƒ:")
        for task, info in stats['task_distribution'].items():
            print(f"      â€¢ {task}: {info['count']} ({info['percentage']:.1f}%)")
        
        # 8. æµ‹è¯•ä»»åŠ¡ç‰¹å®šæ‰¹æ¬¡
        print("\n8ï¸âƒ£ æµ‹è¯•ä»»åŠ¡ç‰¹å®šé‡‡æ ·...")
        if 'sentiment_analysis' in task_aware_dataset.task_indices:
            sentiment_batch = task_aware_dataset.get_task_batch('sentiment_analysis', 2)
            print(f"   ğŸ˜Š æƒ…æ„Ÿåˆ†ææ‰¹æ¬¡: {len(sentiment_batch)} ä¸ªæ ·æœ¬")
        
        # 9. æµ‹è¯•æ··åˆä»»åŠ¡æ‰¹æ¬¡
        mixed_batch = task_aware_dataset.get_mixed_task_batch(3)
        print(f"   ğŸ”€ æ··åˆä»»åŠ¡æ‰¹æ¬¡: {len(mixed_batch)} ä¸ªæ ·æœ¬")
        task_types = [item['task_type'] for item in mixed_batch]
        print(f"      ä»»åŠ¡ç±»å‹: {task_types}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("   ğŸ’¡ æç¤º: ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ŒæŸäº›æ•°æ®é›†éœ€è¦ä»HuggingFaceä¸‹è½½")
        import traceback
        traceback.print_exc()


def demo_training_with_datasets():
    """æ¼”ç¤ºä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒ"""
    print("\n" + "ğŸš€ æ•°æ®é›†è®­ç»ƒæ¼”ç¤º")
    print("=" * 60)
    
    try:
        from src.core.collaborator import MultiModelCollaborator
        from src.training.alignment_trainer import AlignmentTrainer
        from transformers import AutoModel
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤„ç†å¥½çš„æ•°æ®
        data_dir = "./demo_processed_data"
        if not os.path.exists(data_dir):
            print("âŒ æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†æ¼”ç¤º")
            return
        
        print("\n1ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹...")
        model1 = AutoModel.from_pretrained("bert-base-uncased")
        model2 = AutoModel.from_pretrained("gpt2")
        collaborator = MultiModelCollaborator([model1, model2])
        
        print("\n2ï¸âƒ£ é…ç½®è®­ç»ƒå™¨...")
        dataset_config = {
            'data_dir': data_dir,
            'tokenizer_name': 'bert-base-uncased'
        }
        
        trainer = AlignmentTrainer(
            collaborator, 
            learning_rate=1e-4,
            dataset_config=dataset_config
        )
        
        print("\n3ï¸âƒ£ å¼€å§‹è®­ç»ƒ...")
        results = trainer.train_with_dataset_selection(
            dataset_names=['imdb', 'ag_news'],
            epochs=3,  # åªè®­ç»ƒ3ä¸ªepochç”¨äºæ¼”ç¤º
            batch_size=4,  # å°æ‰¹æ¬¡
            max_samples_per_dataset=20,  # æ¯ä¸ªæ•°æ®é›†åªç”¨20ä¸ªæ ·æœ¬
            task_sampling_strategy='balanced',
            validation_split=0.3
        )
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {results['final_train_loss']:.4f}")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.4f}")
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {results['dataset_info']['train_samples']}")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {results['dataset_info']['val_samples']}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒæ¼”ç¤º")
    print("è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºå¦‚ä½•:")
    print("  1. ä¸‹è½½å’Œé¢„å¤„ç†äº’è”ç½‘æ•°æ®é›†")
    print("  2. åˆ›å»ºä»»åŠ¡æ„ŸçŸ¥æ•°æ®é›†")
    print("  3. ä½¿ç”¨æ•°æ®é›†è¿›è¡Œé€‚é…å™¨è®­ç»ƒ")
    print("\nâš ï¸  æ³¨æ„: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")
    
    try:
        # æ¼”ç¤ºæ•°æ®é¢„å¤„ç†
        demo_dataset_preprocessing()
        
        # æ¼”ç¤ºè®­ç»ƒ
        demo_training_with_datasets()
        
        print("\n" + "ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("=" * 60)
        print("æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤ä½¿ç”¨å®Œæ•´åŠŸèƒ½:")
        print("  python main.py data list                    # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ•°æ®é›†")
        print("  python main.py data create-mixed imdb squad # åˆ›å»ºæ··åˆæ•°æ®é›†")
        print("  python main.py train-with-data imdb squad   # ä½¿ç”¨æ•°æ®é›†è®­ç»ƒ")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
