#!/usr/bin/env python3
"""
ä»»åŠ¡æ„ŸçŸ¥å¤šæ¨¡å‹åä½œæ¡†æ¶ä¸»å…¥å£

ä½¿ç”¨æ–¹æ³•:
    python main.py demo basic           # åŸºç¡€æ¼”ç¤º
    python main.py demo training       # è®­ç»ƒæ¼”ç¤º
    python main.py demo comparison     # å¯¹æ¯”æ¼”ç¤º
    python main.py demo interactive    # äº¤äº’å¼æ¼”ç¤º
    python main.py test                # è¿è¡Œæµ‹è¯•
    python main.py quick-start         # å¿«é€Ÿå¼€å§‹
    python main.py data list           # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    python main.py data create-mixed dataset1 dataset2  # åˆ›å»ºæ··åˆæ•°æ®é›†
    python main.py train-with-data dataset1 dataset2 --epochs 10  # ä½¿ç”¨æ•°æ®é›†è®­ç»ƒ
"""

import sys
import os
import argparse

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


def run_demo(mode):
    """è¿è¡Œæ¼”ç¤º"""
    from examples.demo import main as demo_main
    sys.argv = ['demo.py', mode]
    demo_main()


def run_quick_start():
    """è¿è¡Œå¿«é€Ÿå¼€å§‹"""
    from examples.quick_start import quick_start
    quick_start()


def run_tests():
    """è¿è¡Œæµ‹è¯•"""
    import subprocess
    test_files = [
        'tests/task_aware_test.py',
        'tests/test_collaboration.py',
        'tests/quick_test.py'
    ]
    
    for test_file in test_files:
        if os.path.exists(test_file):
            print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_file}")
            try:
                subprocess.run([sys.executable, test_file], check=True)
            except subprocess.CalledProcessError as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {test_file}")
                print(f"é”™è¯¯: {e}")
            except KeyboardInterrupt:
                print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
                break
        else:
            print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")


def show_project_info():
    """æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯"""
    print("ğŸ¤– ä»»åŠ¡æ„ŸçŸ¥å¤šæ¨¡å‹åä½œæ¡†æ¶")
    print("=" * 50)
    print("ç‰ˆæœ¬: 1.0.0")
    print("ä½œè€…: Task-Aware Collaboration Team")
    print()
    print("ğŸ“‹ é¡¹ç›®ç»“æ„:")
    
    # æ˜¾ç¤ºé¡¹ç›®ç»“æ„
    def show_tree(path, prefix="", max_depth=3, current_depth=0):
        if current_depth >= max_depth:
            return
        
        items = []
        try:
            for item in sorted(os.listdir(path)):
                if not item.startswith('.') and item != '__pycache__':
                    items.append(item)
        except PermissionError:
            return
        
        for i, item in enumerate(items):
            is_last = i == len(items) - 1
            item_path = os.path.join(path, item)
            
            if is_last:
                print(f"{prefix}â””â”€â”€ {item}")
                new_prefix = prefix + "    "
            else:
                print(f"{prefix}â”œâ”€â”€ {item}")
                new_prefix = prefix + "â”‚   "
            
            if os.path.isdir(item_path) and not item.startswith('.'):
                show_tree(item_path, new_prefix, max_depth, current_depth + 1)
    
    show_tree(".")
    
    print("\nğŸ“¦ æ ¸å¿ƒç»„ä»¶:")
    print("  â€¢ MultiModelCollaborator: å¤šæ¨¡å‹åä½œç³»ç»Ÿ")
    print("  â€¢ TaskDetector: ä»»åŠ¡ç±»å‹æ£€æµ‹å™¨")
    print("  â€¢ TaskAwareAdapter: ä»»åŠ¡æ„ŸçŸ¥é€‚é…å™¨")
    print("  â€¢ TaskAwareTrainer: ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒå™¨")
    
    print("\nğŸ¯ æ”¯æŒçš„ä»»åŠ¡ç±»å‹:")
    print("  â€¢ é—®ç­” (QA)")
    print("  â€¢ æƒ…æ„Ÿåˆ†æ (Sentiment)")
    print("  â€¢ æ–‡æœ¬ç”Ÿæˆ (Generation)")
    print("  â€¢ å¯¹è¯ (Conversation)")
    print("  â€¢ é€šç”¨ä»»åŠ¡ (General)")


def run_data_command(args):
    """è¿è¡Œæ•°æ®ç›¸å…³å‘½ä»¤"""
    import subprocess
    
    # æ„å»ºå‘½ä»¤
    cmd = [sys.executable, 'scripts/preprocess_data.py', args.data_command]
    
    if hasattr(args, 'datasets') and args.datasets:
        cmd.extend(args.datasets)
    
    # åªä¸ºéœ€è¦è¿™äº›å‚æ•°çš„å‘½ä»¤æ·»åŠ å‚æ•°
    if args.data_command in ['create-mixed', 'preprocess-task', 'preprocess-adapter']:
        if hasattr(args, 'max_samples_per_dataset') and args.max_samples_per_dataset:
            cmd.extend(['--max-samples-per-dataset', str(args.max_samples_per_dataset)])
        
        if hasattr(args, 'output_dir') and args.output_dir:
            cmd.extend(['--output-dir', args.output_dir])
        
        if hasattr(args, 'split') and args.split:
            cmd.extend(['--split', args.split])
    
    # æ‰§è¡Œå‘½ä»¤
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        print(f"âŒ æ•°æ®å¤„ç†å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")


def train_with_datasets(args):
    """ä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒ"""
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨æ•°æ®é›†è®­ç»ƒ: {args.datasets}")
    
    from src.core.collaborator import MultiModelCollaborator
    from src.training.alignment_trainer import AlignmentTrainer
    from transformers import AutoModel
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        print("åˆå§‹åŒ–æ¨¡å‹...")
        model1 = AutoModel.from_pretrained("bert-base-uncased")
        model2 = AutoModel.from_pretrained("gpt2")
        collaborator = MultiModelCollaborator([model1, model2])
        
        # é…ç½®æ•°æ®é›†
        dataset_config = {
            'data_dir': args.data_dir,
            'tokenizer_name': args.tokenizer
        }
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = AlignmentTrainer(
            collaborator, 
            learning_rate=args.learning_rate,
            dataset_config=dataset_config
        )
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train_with_dataset_selection(
            dataset_names=args.datasets,
            epochs=args.epochs,
            batch_size=args.batch_size,
            max_samples_per_dataset=args.max_samples_per_dataset,
            task_sampling_strategy=args.sampling_strategy,
            validation_split=args.validation_split
        )
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {results['final_train_loss']:.4f}")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.4f}")
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {results['dataset_info']['train_samples']}")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {results['dataset_info']['val_samples']}")
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        import json
        from pathlib import Path
        
        results_dir = Path('./results')
        results_dir.mkdir(exist_ok=True)
        
        results_file = results_dir / 'dataset_training_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"   è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä»»åŠ¡æ„ŸçŸ¥å¤šæ¨¡å‹åä½œæ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py info                    # æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯
  python main.py demo basic             # åŸºç¡€æ¼”ç¤º
  python main.py demo training          # è®­ç»ƒæ¼”ç¤º
  python main.py demo comparison        # å¯¹æ¯”æ¼”ç¤º
  python main.py demo interactive       # äº¤äº’å¼æ¼”ç¤º
  python main.py test                   # è¿è¡Œæµ‹è¯•
  python main.py quick-start            # å¿«é€Ÿå¼€å§‹
  python main.py data list              # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
  python main.py data create-mixed imdb ag_news --output-dir ./data  # åˆ›å»ºæ··åˆæ•°æ®é›†
  python main.py train-with-data imdb ag_news --epochs 10            # ä½¿ç”¨æ•°æ®é›†è®­ç»ƒ
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # infoå‘½ä»¤
    subparsers.add_parser('info', help='æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯')
    
    # demoå‘½ä»¤
    demo_parser = subparsers.add_parser('demo', help='è¿è¡Œæ¼”ç¤º')
    demo_parser.add_argument(
        'mode', 
        choices=['basic', 'training', 'comparison', 'interactive'],
        help='æ¼”ç¤ºæ¨¡å¼'
    )
    
    # testå‘½ä»¤
    subparsers.add_parser('test', help='è¿è¡Œæµ‹è¯•')
    
    # quick-startå‘½ä»¤
    subparsers.add_parser('quick-start', help='å¿«é€Ÿå¼€å§‹')
    
    # dataå‘½ä»¤
    data_parser = subparsers.add_parser('data', help='æ•°æ®å¤„ç†å‘½ä»¤')
    data_parser.add_argument(
        'data_command',
        choices=['list', 'create-mixed', 'preprocess-task', 'preprocess-adapter', 'validate', 'stats'],
        help='æ•°æ®å¤„ç†æ“ä½œ'
    )
    data_parser.add_argument('datasets', nargs='*', help='æ•°æ®é›†åç§°åˆ—è¡¨')
    data_parser.add_argument('--max-samples-per-dataset', type=int, default=1000, help='æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°')
    data_parser.add_argument('--output-dir', default='./processed_data', help='è¾“å‡ºç›®å½•')
    data_parser.add_argument('--split', default='train', help='æ•°æ®é›†åˆ†å‰²')
    
    # train-with-dataå‘½ä»¤
    train_parser = subparsers.add_parser('train-with-data', help='ä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒ')
    train_parser.add_argument('datasets', nargs='+', help='æ•°æ®é›†åç§°åˆ—è¡¨')
    train_parser.add_argument('--data-dir', default='./processed_data', help='æ•°æ®ç›®å½•')
    train_parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    train_parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    train_parser.add_argument('--learning-rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    train_parser.add_argument('--max-samples-per-dataset', type=int, default=1000, help='æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°')
    train_parser.add_argument('--sampling-strategy', default='balanced', 
                            choices=['balanced', 'proportional', 'random'], help='ä»»åŠ¡é‡‡æ ·ç­–ç•¥')
    train_parser.add_argument('--validation-split', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹')
    train_parser.add_argument('--tokenizer', default='bert-base-uncased', help='åˆ†è¯å™¨åç§°')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == 'info':
            show_project_info()
        elif args.command == 'demo':
            run_demo(args.mode)
        elif args.command == 'test':
            run_tests()
        elif args.command == 'quick-start':
            run_quick_start()
        elif args.command == 'data':
            run_data_command(args)
        elif args.command == 'train-with-data':
            train_with_datasets(args)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
