#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆåä½œæµ‹è¯•è„šæœ¬ï¼šæ˜¾ç¤ºè®­ç»ƒå‰åçš„å®é™…è¾“å‡ºå·®å¼‚
"""

import torch
import torch.nn.functional as F
import numpy as np
from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel, AutoModelForSequenceClassification
from Multi import MultiModelCollaborator, AlignmentTrainer, AlignmentEvaluator

def get_text_generation_output(collaborator, text, use_collaboration=False):
    """è·å–æ–‡æœ¬ç”Ÿæˆçš„å®é™…è¾“å‡º"""
    if not hasattr(collaborator, 'gpt2_generator'):
        # åˆ›å»ºä¸€ä¸ªGPT-2ç”Ÿæˆæ¨¡å‹
        collaborator.gpt2_generator = GPT2LMHeadModel.from_pretrained("gpt2")
    
    tokenizer = collaborator.tokenizers[1]  # GPT-2 tokenizer
    
    if use_collaboration:
        # ä½¿ç”¨åä½œï¼šä»BERTè·å–ä¿¡æ¯ï¼Œä¼ é€’ç»™GPT-2
        collaboration_output = collaborator.collaborate(text, 0, 1)
        adapted_hidden = collaboration_output['adapted_hidden']
        
        # ä½¿ç”¨é€‚é…åçš„hidden statesä½œä¸ºåˆå§‹çŠ¶æ€
        inputs = tokenizer.encode_plus(text, return_tensors="pt", padding=True, truncation=True)
        
        # ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨åä½œä¿¡æ¯ï¼‰
        with torch.no_grad():
            # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿä½¿ç”¨åä½œä¿¡æ¯å½±å“ç”Ÿæˆ
            outputs = collaborator.gpt2_generator.generate(
                inputs['input_ids'],
                max_length=len(inputs['input_ids'][0]) + 10,
                num_return_sequences=1,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
    else:
        # æ­£å¸¸ç”Ÿæˆ
        inputs = tokenizer.encode_plus(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = collaborator.gpt2_generator.generate(
                inputs['input_ids'],
                max_length=len(inputs['input_ids'][0]) + 10,
                num_return_sequences=1,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

def get_classification_output(collaborator, text, use_collaboration=False):
    """è·å–åˆ†ç±»ä»»åŠ¡çš„å®é™…è¾“å‡º"""
    # ä½¿ç”¨ç®€å•çš„æ–¹æ³•ï¼šåŸºäºhidden statesçš„æœ€åä¸€å±‚è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»
    if use_collaboration:
        # ä½¿ç”¨åä½œåçš„hidden states
        collaboration_output = collaborator.collaborate(text, 0, 1)
        hidden_states = collaboration_output['adapted_hidden']
    else:
        # ä½¿ç”¨åŸå§‹BERTçš„hidden states
        hidden_states = collaborator.get_hidden_states(text, 0)
    
    # ç®€å•çš„æƒ…æ„Ÿåˆ†ç±»ï¼šåŸºäºhidden statesçš„å¹³å‡å€¼
    pooled = hidden_states.mean(dim=1)  # [1, hidden_size]
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰
    if not hasattr(collaborator, 'classifier'):
        hidden_size = hidden_states.size(-1)
        collaborator.classifier = torch.nn.Linear(hidden_size, 3)  # 3ä¸ªç±»åˆ«ï¼šæ­£é¢ã€ä¸­æ€§ã€è´Ÿé¢
    
    with torch.no_grad():
        logits = collaborator.classifier(pooled)
        probabilities = F.softmax(logits, dim=-1)
    
    labels = ['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢']
    predicted_class = torch.argmax(probabilities, dim=-1).item()
    confidence = probabilities[0][predicted_class].item()
    
    return {
        'predicted_label': labels[predicted_class],
        'confidence': confidence,
        'probabilities': {labels[i]: probabilities[0][i].item() for i in range(3)}
    }

def compare_similarity_understanding(collaborator, text1, text2, use_collaboration=False):
    """æ¯”è¾ƒæ¨¡å‹å¯¹è¯­ä¹‰ç›¸ä¼¼æ€§çš„ç†è§£"""
    if use_collaboration:
        # ä½¿ç”¨åä½œåçš„æŠ•å½±
        hidden1_1 = collaborator.get_hidden_states(text1, 0)
        hidden2_1 = collaborator.get_hidden_states(text1, 1)
        hidden1_2 = collaborator.get_hidden_states(text2, 0)
        hidden2_2 = collaborator.get_hidden_states(text2, 1)
        
        proj1_1, proj2_1 = collaborator.central_processor.process([hidden1_1, hidden2_1])
        proj1_2, proj2_2 = collaborator.central_processor.process([hidden1_2, hidden2_2])
        
        # è®¡ç®—BERTæŠ•å½±é—´çš„ç›¸ä¼¼æ€§
        bert_similarity = F.cosine_similarity(
            proj1_1.mean(dim=1), proj1_2.mean(dim=1)
        ).item()
        
        # è®¡ç®—è·¨æ¨¡å‹ä¸€è‡´æ€§
        cross_consistency = F.cosine_similarity(
            proj1_1.mean(dim=1), proj2_1.mean(dim=1)
        ).item()
        
        return {
            'bert_projection_similarity': bert_similarity,
            'cross_model_consistency': cross_consistency,
            'interpretation': f"åä½œåæ¨¡å‹è®¤ä¸ºè¿™ä¸¤å¥è¯çš„ç›¸ä¼¼åº¦æ˜¯ {bert_similarity:.3f}"
        }
    else:
        # ä½¿ç”¨åŸå§‹hidden states
        hidden1_bert = collaborator.get_hidden_states(text1, 0)
        hidden2_bert = collaborator.get_hidden_states(text2, 0)
        hidden1_gpt = collaborator.get_hidden_states(text1, 1)
        hidden2_gpt = collaborator.get_hidden_states(text2, 1)
        
        bert_similarity = F.cosine_similarity(
            hidden1_bert.mean(dim=1), hidden2_bert.mean(dim=1)
        ).item()
        
        gpt_similarity = F.cosine_similarity(
            hidden1_gpt.mean(dim=1), hidden2_gpt.mean(dim=1)
        ).item()
        
        return {
            'bert_similarity': bert_similarity,
            'gpt_similarity': gpt_similarity,
            'interpretation': f"BERTè®¤ä¸ºç›¸ä¼¼åº¦æ˜¯ {bert_similarity:.3f}, GPT-2è®¤ä¸ºæ˜¯ {gpt_similarity:.3f}"
        }

def get_attention_focus(collaborator, text, model_idx):
    """è·å–æ¨¡å‹æ³¨æ„åŠ›çš„ç„¦ç‚¹è¯æ±‡"""
    tokenizer = collaborator.tokenizers[model_idx]
    inputs = tokenizer.encode_plus(text, return_tensors="pt", padding=True, truncation=True)
    
    with torch.no_grad():
        outputs = collaborator.models[model_idx](**inputs, output_attentions=True)
    
    # è·å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡
    attention = outputs.attentions[-1]  # [batch, heads, seq_len, seq_len]
    
    # å¹³å‡æ‰€æœ‰æ³¨æ„åŠ›å¤´
    avg_attention = attention.mean(dim=1)[0]  # [seq_len, seq_len]
    
    # è·å–æ¯ä¸ªtokenå¯¹æ‰€æœ‰å…¶ä»–tokençš„å¹³å‡æ³¨æ„åŠ›
    token_importance = avg_attention.mean(dim=0)
    
    # è·å–tokens
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    
    # æ‰¾åˆ°æœ€é‡è¦çš„3ä¸ªtokensï¼ˆæ’é™¤ç‰¹æ®Štokensï¼‰
    important_indices = []
    for i, token in enumerate(tokens):
        if token not in ['[CLS]', '[SEP]', '<|endoftext|>'] and not token.startswith('Ä '):
            important_indices.append((i, token, token_importance[i].item()))
    
    # æŒ‰é‡è¦æ€§æ’åº
    important_indices.sort(key=lambda x: x[2], reverse=True)
    
    return {
        'tokens': tokens,
        'most_important': important_indices[:3] if len(important_indices) >= 3 else important_indices,
        'attention_pattern': avg_attention.cpu().numpy()
    }

def quick_test():
    """å¿«é€Ÿæµ‹è¯•å‡½æ•° - æ˜¾ç¤ºå®é™…çš„ä»»åŠ¡è¾“å‡ºå·®å¼‚"""
    print("ğŸš€ å¤šæ¨¡å‹åä½œæ•ˆæœæµ‹è¯• - å®é™…è¾“å‡ºå¯¹æ¯”")
    print("=" * 60)
    
    # åˆå§‹åŒ–
    model1 = AutoModel.from_pretrained("bert-base-uncased")
    model2 = AutoModel.from_pretrained("gpt2")
    collaborator = MultiModelCollaborator([model1, model2])
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "The weather is",
        "Artificial intelligence will",
        "I feel happy because"
    ]
    
    similarity_pairs = [
        ("The cat is sleeping", "A feline is resting"),
        ("I love programming", "I enjoy coding")
    ]
    
    print("\nğŸ” ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒå‰çš„å®é™…è¾“å‡º")
    print("=" * 60)
    
    # 1. æ–‡æœ¬ç”Ÿæˆå¯¹æ¯”
    print("\nğŸ“ æ–‡æœ¬ç”Ÿæˆä»»åŠ¡:")
    generation_before = {}
    for text in test_texts:
        normal_output = get_text_generation_output(collaborator, text, use_collaboration=False)
        collab_output = get_text_generation_output(collaborator, text, use_collaboration=True)
        
        generation_before[text] = {
            'normal': normal_output,
            'collaborative': collab_output
        }
        
        print(f"\nè¾“å…¥: '{text}'")
        print(f"  æ­£å¸¸ç”Ÿæˆ: {normal_output}")
        print(f"  åä½œç”Ÿæˆ: {collab_output}")
    
    # 2. æƒ…æ„Ÿåˆ†æå¯¹æ¯”
    print(f"\nğŸ˜Š æƒ…æ„Ÿåˆ†æä»»åŠ¡:")
    sentiment_before = {}
    sentiment_texts = ["I love this movie", "This is terrible", "The weather is okay"]
    
    for text in sentiment_texts:
        normal_sentiment = get_classification_output(collaborator, text, use_collaboration=False)
        collab_sentiment = get_classification_output(collaborator, text, use_collaboration=True)
        
        sentiment_before[text] = {
            'normal': normal_sentiment,
            'collaborative': collab_sentiment
        }
        
        print(f"\næ–‡æœ¬: '{text}'")
        print(f"  æ­£å¸¸åˆ†ç±»: {normal_sentiment['predicted_label']} (ç½®ä¿¡åº¦: {normal_sentiment['confidence']:.3f})")
        print(f"  åä½œåˆ†ç±»: {collab_sentiment['predicted_label']} (ç½®ä¿¡åº¦: {collab_sentiment['confidence']:.3f})")
    
    # 3. è¯­ä¹‰ç›¸ä¼¼æ€§ç†è§£
    print(f"\nğŸ”„ è¯­ä¹‰ç›¸ä¼¼æ€§ç†è§£:")
    similarity_before = {}
    
    for text1, text2 in similarity_pairs:
        normal_sim = compare_similarity_understanding(collaborator, text1, text2, use_collaboration=False)
        collab_sim = compare_similarity_understanding(collaborator, text1, text2, use_collaboration=True)
        
        similarity_before[(text1, text2)] = {
            'normal': normal_sim,
            'collaborative': collab_sim
        }
        
        print(f"\nå¯¹æ¯”: '{text1}' vs '{text2}'")
        print(f"  è®­ç»ƒå‰: {normal_sim['interpretation']}")
        print(f"  åä½œå: {collab_sim['interpretation']}")
    
    # è®­ç»ƒé€‚é…å™¨
    print("\nğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒé€‚é…å™¨")
    print("=" * 60)
    trainer = AlignmentTrainer(collaborator, learning_rate=1e-4)
    
    train_texts = test_texts + sentiment_texts + [t for pair in similarity_pairs for t in pair]
    train_texts.extend([
        "Machine learning is powerful",
        "The ocean is vast and deep",
        "Music brings joy to people"
    ])
    
    for epoch in range(3):
        loss = trainer.train_epoch(train_texts)
        print(f"  Epoch {epoch+1}: Loss = {loss:.4f}")
    
    print("\nğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šè®­ç»ƒåçš„å®é™…è¾“å‡º")
    print("=" * 60)
    
    # é‡æ–°æµ‹è¯•æ‰€æœ‰ä»»åŠ¡
    print("\nğŸ“ è®­ç»ƒåæ–‡æœ¬ç”Ÿæˆ:")
    generation_after = {}
    for text in test_texts:
        normal_output = get_text_generation_output(collaborator, text, use_collaboration=False)
        collab_output = get_text_generation_output(collaborator, text, use_collaboration=True)
        
        generation_after[text] = {
            'normal': normal_output,
            'collaborative': collab_output
        }
        
        print(f"\nè¾“å…¥: '{text}'")
        print(f"  æ­£å¸¸ç”Ÿæˆ: {normal_output}")
        print(f"  åä½œç”Ÿæˆ: {collab_output}")
        
        # æ˜¾ç¤ºå˜åŒ–
        if text in generation_before:
            print(f"  ğŸ“Š å˜åŒ–:")
            print(f"    åä½œç”Ÿæˆå‰: {generation_before[text]['collaborative']}")
            print(f"    åä½œç”Ÿæˆå: {collab_output}")
    
    print(f"\nğŸ˜Š è®­ç»ƒåæƒ…æ„Ÿåˆ†æ:")
    for text in sentiment_texts:
        normal_sentiment = get_classification_output(collaborator, text, use_collaboration=False)
        collab_sentiment = get_classification_output(collaborator, text, use_collaboration=True)
        
        print(f"\næ–‡æœ¬: '{text}'")
        print(f"  æ­£å¸¸åˆ†ç±»: {normal_sentiment['predicted_label']} (ç½®ä¿¡åº¦: {normal_sentiment['confidence']:.3f})")
        print(f"  åä½œåˆ†ç±»: {collab_sentiment['predicted_label']} (ç½®ä¿¡åº¦: {collab_sentiment['confidence']:.3f})")
        
        if text in sentiment_before:
            print(f"  ğŸ“Š ç½®ä¿¡åº¦å˜åŒ–:")
            print(f"    è®­ç»ƒå‰: {sentiment_before[text]['collaborative']['confidence']:.3f}")
            print(f"    è®­ç»ƒå: {collab_sentiment['confidence']:.3f}")
    
    print(f"\nğŸ”„ è®­ç»ƒåè¯­ä¹‰ç›¸ä¼¼æ€§:")
    for text1, text2 in similarity_pairs:
        normal_sim = compare_similarity_understanding(collaborator, text1, text2, use_collaboration=False)
        collab_sim = compare_similarity_understanding(collaborator, text1, text2, use_collaboration=True)
        
        print(f"\nå¯¹æ¯”: '{text1}' vs '{text2}'")
        print(f"  è®­ç»ƒå: {collab_sim['interpretation']}")
        
        if (text1, text2) in similarity_before:
            before_cross = similarity_before[(text1, text2)]['collaborative'].get('cross_model_consistency', 0)
            after_cross = collab_sim.get('cross_model_consistency', 0)
            print(f"  ğŸ“Š è·¨æ¨¡å‹ä¸€è‡´æ€§å˜åŒ–: {before_cross:.3f} â†’ {after_cross:.3f}")
    
    print("\nâœ¨ ç¬¬å››é˜¶æ®µï¼šå…³é”®æ”¹è¿›æ€»ç»“")
    print("=" * 60)
    
    print("ğŸ¯ ä¸»è¦è§‚å¯Ÿ:")
    print("  1. æ–‡æœ¬ç”Ÿæˆçš„åˆ›é€ æ€§å’Œä¸€è‡´æ€§")
    print("  2. æƒ…æ„Ÿåˆ†æçš„å‡†ç¡®æ€§å’Œç½®ä¿¡åº¦")
    print("  3. è¯­ä¹‰ç†è§£çš„è·¨æ¨¡å‹ä¸€è‡´æ€§")
    print("  4. æ¨¡å‹é—´ä¿¡æ¯ä¼ é€’çš„æœ‰æ•ˆæ€§")
    
    return {
        'generation_before': generation_before,
        'generation_after': generation_after,
        'sentiment_before': sentiment_before,
        'similarity_before': similarity_before
    }

import torch
import torch.nn.functional as F
import numpy as np
from transformers import AutoModel, AutoTokenizer
from Multi import MultiModelCollaborator, AlignmentTrainer, AlignmentEvaluator

def analyze_hidden_states(hidden_states, name):
    """åˆ†æhidden statesçš„è¯¦ç»†ç‰¹å¾"""
    print(f"\nï¿½ {name} Hidden States åˆ†æ:")
    print(f"  å½¢çŠ¶: {hidden_states.shape}")
    print(f"  å¹³å‡å€¼: {hidden_states.mean().item():.6f}")
    print(f"  æ ‡å‡†å·®: {hidden_states.std().item():.6f}")
    print(f"  æœ€å¤§å€¼: {hidden_states.max().item():.6f}")
    print(f"  æœ€å°å€¼: {hidden_states.min().item():.6f}")
    
    # è®¡ç®—æ¿€æ´»ç¥ç»å…ƒæ¯”ä¾‹ï¼ˆç»å¯¹å€¼å¤§äº0.1çš„ï¼‰
    active_ratio = (torch.abs(hidden_states) > 0.1).float().mean().item()
    print(f"  æ¿€æ´»ç¥ç»å…ƒæ¯”ä¾‹: {active_ratio:.3f}")
    
    return {
        'shape': list(hidden_states.shape),
        'mean': hidden_states.mean().item(),
        'std': hidden_states.std().item(),
        'max': hidden_states.max().item(),
        'min': hidden_states.min().item(),
        'active_ratio': active_ratio
    }

def get_top_tokens_and_attention(collaborator, text, model_idx):
    """è·å–æ¨¡å‹çš„Topæ¿€æ´»tokenså’Œæ³¨æ„åŠ›æƒé‡"""
    tokenizer = collaborator.tokenizers[model_idx]
    
    # è·å–tokenåŒ–ç»“æœ
    inputs = tokenizer.encode_plus(text, return_tensors="pt", padding=True, truncation=True)
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    
    # è·å–hidden states
    with torch.no_grad():
        outputs = collaborator.models[model_idx](**inputs, output_hidden_states=True, output_attentions=True)
    
    hidden_states = outputs.hidden_states[-1]  # æœ€åä¸€å±‚
    attentions = outputs.attentions[-1]  # æœ€åä¸€å±‚æ³¨æ„åŠ›
    
    # è®¡ç®—æ¯ä¸ªtokençš„é‡è¦æ€§ï¼ˆé€šè¿‡hidden stateçš„normï¼‰
    token_importance = torch.norm(hidden_states[0], dim=-1)
    
    # è·å–æœ€é‡è¦çš„tokens
    top_indices = torch.argsort(token_importance, descending=True)[:3]
    
    result = {
        'tokens': tokens,
        'top_tokens': [(tokens[i], token_importance[i].item()) for i in top_indices],
        'attention_avg': attentions.mean(dim=1)[0].cpu().numpy()  # å¹³å‡æ³¨æ„åŠ›æƒé‡
    }
    
    return result, hidden_states

def compare_model_outputs(collaborator, text):
    """è¯¦ç»†å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡º"""
    print(f"\nğŸ” è¯¦ç»†åˆ†ææ–‡æœ¬: '{text}'")
    print("=" * 60)
    
    # è·å–ä¸¤ä¸ªæ¨¡å‹çš„è¯¦ç»†è¾“å‡º
    model1_analysis, hidden1 = get_top_tokens_and_attention(collaborator, text, 0)
    model2_analysis, hidden2 = get_top_tokens_and_attention(collaborator, text, 1)
    
    print(f"\nğŸ“ BERT (Model 1) åˆ†æ:")
    print(f"  Tokens: {model1_analysis['tokens']}")
    print(f"  é‡è¦tokens: {model1_analysis['top_tokens']}")
    
    print(f"\nğŸ“ GPT-2 (Model 2) åˆ†æ:")
    print(f"  Tokens: {model2_analysis['tokens']}")
    print(f"  é‡è¦tokens: {model2_analysis['top_tokens']}")
    
    # åˆ†æåŸå§‹hidden states
    bert_stats = analyze_hidden_states(hidden1, "BERT")
    gpt_stats = analyze_hidden_states(hidden2, "GPT-2")
    
    # é€šè¿‡åä½œç³»ç»Ÿå¤„ç†
    collaboration_output = collaborator.collaborate(text, 0, 1)
    adapted_hidden = collaboration_output['adapted_hidden']
    
    print(f"\nğŸ”„ åä½œå (BERTâ†’GPT-2) åˆ†æ:")
    adapted_stats = analyze_hidden_states(adapted_hidden, "åä½œé€‚é…")
    
    # è®¡ç®—å˜åŒ–
    print(f"\nğŸ“Š å…³é”®å˜åŒ–:")
    print(f"  ç»´åº¦å˜åŒ–: {bert_stats['shape']} â†’ {adapted_stats['shape']}")
    print(f"  æ¿€æ´»å¼ºåº¦å˜åŒ–: {bert_stats['mean']:.6f} â†’ {adapted_stats['mean']:.6f}")
    print(f"  ä¿¡æ¯å¯†åº¦å˜åŒ–: {bert_stats['std']:.6f} â†’ {adapted_stats['std']:.6f}")
    
    return {
        'bert': bert_stats,
        'gpt': gpt_stats, 
        'adapted': adapted_stats,
        'bert_tokens': model1_analysis,
        'gpt_tokens': model2_analysis
    }

def semantic_similarity_test(collaborator, text1, text2):
    """æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼æ€§åœ¨åä½œå‰åçš„å˜åŒ–"""
    print(f"\nğŸ”„ è¯­ä¹‰ç›¸ä¼¼æ€§æµ‹è¯•:")
    print(f"  æ–‡æœ¬1: '{text1}'")
    print(f"  æ–‡æœ¬2: '{text2}'")
    
    # è·å–åŸå§‹hidden states
    hidden1_1 = collaborator.get_hidden_states(text1, 0)  # BERT
    hidden1_2 = collaborator.get_hidden_states(text2, 0)  # BERT
    hidden2_1 = collaborator.get_hidden_states(text1, 1)  # GPT-2
    hidden2_2 = collaborator.get_hidden_states(text2, 1)  # GPT-2
    
    # è®¡ç®—åŸå§‹ç›¸ä¼¼æ€§
    bert_sim = F.cosine_similarity(
        hidden1_1.mean(dim=1), hidden1_2.mean(dim=1)
    ).item()
    
    gpt_sim = F.cosine_similarity(
        hidden2_1.mean(dim=1), hidden2_2.mean(dim=1)
    ).item()
    
    cross_sim_before = F.cosine_similarity(
        hidden1_1.mean(dim=1), hidden2_1.mean(dim=1)
    ).item()
    
    # é€šè¿‡åä½œç³»ç»Ÿå¤„ç†
    proj1_1, proj2_1 = collaborator.central_processor.process([hidden1_1, hidden2_1])
    proj1_2, proj2_2 = collaborator.central_processor.process([hidden1_2, hidden2_2])
    
    # è®¡ç®—åä½œåç›¸ä¼¼æ€§
    proj_sim1 = F.cosine_similarity(
        proj1_1.mean(dim=1), proj1_2.mean(dim=1)
    ).item()
    
    cross_sim_after = F.cosine_similarity(
        proj1_1.mean(dim=1), proj2_1.mean(dim=1)
    ).item()
    
    print(f"  BERTå†…éƒ¨ç›¸ä¼¼æ€§: {bert_sim:.4f}")
    print(f"  GPT-2å†…éƒ¨ç›¸ä¼¼æ€§: {gpt_sim:.4f}")
    print(f"  è·¨æ¨¡å‹ç›¸ä¼¼æ€§ (åä½œå‰): {cross_sim_before:.4f}")
    print(f"  è·¨æ¨¡å‹ç›¸ä¼¼æ€§ (åä½œå): {cross_sim_after:.4f}")
    print(f"  åä½œåæŠ•å½±ç›¸ä¼¼æ€§: {proj_sim1:.4f}")
    
    return {
        'bert_similarity': bert_sim,
        'gpt_similarity': gpt_sim,
        'cross_before': cross_sim_before,
        'cross_after': cross_sim_after,
        'projection_similarity': proj_sim1
    }

def quick_test():
    """å¿«é€Ÿæµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¿«é€Ÿåä½œæ•ˆæœæµ‹è¯• - åŒ…å«å®é™…è¾“å‡ºå¯¹æ¯”")
    print("=" * 60)
    
    # åˆå§‹åŒ–
    model1 = AutoModel.from_pretrained("bert-base-uncased")
    model2 = AutoModel.from_pretrained("gpt2")
    collaborator = MultiModelCollaborator([model1, model2])
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "What is artificial intelligence?",
        "The weather is beautiful today",
        "I love reading books"
    ]
    
    print("\nğŸ” ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒå‰çš„è¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªæµ‹è¯•æ–‡æœ¬ï¼ˆè®­ç»ƒå‰ï¼‰
    before_analyses = []
    for text in test_texts:
        analysis = compare_model_outputs(collaborator, text)
        before_analyses.append(analysis)
    
    # è¯­ä¹‰ç›¸ä¼¼æ€§æµ‹è¯•ï¼ˆè®­ç»ƒå‰ï¼‰
    semantic_before = semantic_similarity_test(
        collaborator, 
        "The cat is sleeping", 
        "A feline is resting"
    )
    
    # è®­ç»ƒé€‚é…å™¨
    print("\nğŸ”§ ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒé€‚é…å™¨")
    print("=" * 60)
    trainer = AlignmentTrainer(collaborator, learning_rate=1e-4)
    
    train_texts = test_texts + [
        "Machine learning is powerful",
        "The ocean is vast and deep", 
        "Music brings joy to people",
        "Technology changes our lives",
        "Education opens new doors"
    ]
    
    for epoch in range(3):
        loss = trainer.train_epoch(train_texts)
        print(f"  Epoch {epoch+1}: Loss = {loss:.4f}")
    
    print("\nğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šè®­ç»ƒåçš„è¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªæµ‹è¯•æ–‡æœ¬ï¼ˆè®­ç»ƒåï¼‰
    after_analyses = []
    for text in test_texts:
        analysis = compare_model_outputs(collaborator, text)
        after_analyses.append(analysis)
    
    # è¯­ä¹‰ç›¸ä¼¼æ€§æµ‹è¯•ï¼ˆè®­ç»ƒåï¼‰
    semantic_after = semantic_similarity_test(
        collaborator,
        "The cat is sleeping", 
        "A feline is resting"
    )
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\nğŸ“‹ ç¬¬å››é˜¶æ®µï¼šè®­ç»ƒå‰åå¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    
    print(f"\nğŸ“Š æ•´ä½“æ•°å€¼å˜åŒ–:")
    for i, text in enumerate(test_texts):
        before = before_analyses[i]
        after = after_analyses[i]
        
        print(f"\n  æ–‡æœ¬: '{text}'")
        print(f"    åä½œé€‚é…å¹³å‡å€¼: {before['adapted']['mean']:.6f} â†’ {after['adapted']['mean']:.6f}")
        print(f"    åä½œé€‚é…æ ‡å‡†å·®: {before['adapted']['std']:.6f} â†’ {after['adapted']['std']:.6f}")
        print(f"    æ¿€æ´»ç¥ç»å…ƒæ¯”ä¾‹: {before['adapted']['active_ratio']:.3f} â†’ {after['adapted']['active_ratio']:.3f}")
    
    print(f"\nğŸ”„ è¯­ä¹‰ç†è§£èƒ½åŠ›å˜åŒ–:")
    print(f"  è·¨æ¨¡å‹è¯­ä¹‰ä¸€è‡´æ€§: {semantic_before['cross_before']:.4f} â†’ {semantic_after['cross_after']:.4f}")
    print(f"  æŠ•å½±åè¯­ä¹‰ä¿æŒ: {semantic_before['projection_similarity']:.4f} â†’ {semantic_after['projection_similarity']:.4f}")
    
    # è®¡ç®—æ•´ä½“æ”¹å–„
    cross_improvement = ((semantic_after['cross_after'] - semantic_before['cross_before']) / 
                        abs(semantic_before['cross_before'])) * 100
    
    print(f"\nâœ¨ æ ¸å¿ƒæ”¹å–„:")
    print(f"  è·¨æ¨¡å‹ç†è§£æå‡: {cross_improvement:+.1f}%")
    
    if cross_improvement > 1000:
        conclusion = "ğŸ‰ è®­ç»ƒæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹åä½œæ•ˆæœï¼"
    elif cross_improvement > 100:
        conclusion = "âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½"
    else:
        conclusion = "ğŸ“ˆ æœ‰ä¸€å®šæ”¹å–„"
    
    print(f"  {conclusion}")
    
    return {
        'before_analyses': before_analyses,
        'after_analyses': after_analyses,
        'semantic_before': semantic_before,
        'semantic_after': semantic_after,
        'improvement': cross_improvement
    }

def save_detailed_results(results):
    """ä¿å­˜è¯¦ç»†çš„å¯¹æ¯”ç»“æœåˆ°æ–‡ä»¶"""
    import json
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        "summary": {
            "improvement_percent": results['improvement'],
            "conclusion": "è®­ç»ƒæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹åä½œæ•ˆæœ" if results['improvement'] > 1000 else "è®­ç»ƒæœ‰ä¸€å®šæ•ˆæœ"
        },
        "semantic_analysis": {
            "before": results['semantic_before'],
            "after": results['semantic_after']
        },
        "detailed_changes": []
    }
    
    # æ·»åŠ æ¯ä¸ªæ–‡æœ¬çš„è¯¦ç»†å˜åŒ–
    test_texts = [
        "What is artificial intelligence?",
        "The weather is beautiful today", 
        "I love reading books"
    ]
    
    for i, text in enumerate(test_texts):
        before = results['before_analyses'][i]
        after = results['after_analyses'][i]
        
        change_data = {
            "text": text,
            "before_stats": before['adapted'],
            "after_stats": after['adapted'],
            "bert_tokens": before['bert_tokens']['tokens'],
            "gpt_tokens": before['gpt_tokens']['tokens']
        }
        save_data["detailed_changes"].append(change_data)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('detailed_collaboration_results.json', 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° detailed_collaboration_results.json")
    
    # åŒæ—¶ç”Ÿæˆä¸€ä¸ªç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
    with open('collaboration_report.txt', 'w', encoding='utf-8') as f:
        f.write("ğŸ¤– å¤šæ¨¡å‹åä½œæ•ˆæœè¯¦ç»†æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"ğŸ“Š æ•´ä½“æ”¹å–„: {results['improvement']:+.1f}%\n\n")
        
        f.write("ğŸ“ å„æ–‡æœ¬çš„å…·ä½“å˜åŒ–:\n")
        for i, text in enumerate(test_texts):
            before = results['before_analyses'][i]
            after = results['after_analyses'][i]
            
            f.write(f"\næ–‡æœ¬: '{text}'\n")
            f.write(f"  åä½œé€‚é…å¹³å‡å€¼: {before['adapted']['mean']:.6f} â†’ {after['adapted']['mean']:.6f}\n")
            f.write(f"  åä½œé€‚é…æ ‡å‡†å·®: {before['adapted']['std']:.6f} â†’ {after['adapted']['std']:.6f}\n")
            f.write(f"  æ¿€æ´»ç¥ç»å…ƒæ¯”ä¾‹: {before['adapted']['active_ratio']:.3f} â†’ {after['adapted']['active_ratio']:.3f}\n")
        
        f.write(f"\nğŸ”„ è¯­ä¹‰ç†è§£å˜åŒ–:\n")
        f.write(f"  è·¨æ¨¡å‹è¯­ä¹‰ä¸€è‡´æ€§: {results['semantic_before']['cross_before']:.4f} â†’ {results['semantic_after']['cross_after']:.4f}\n")
        f.write(f"  æŠ•å½±åè¯­ä¹‰ä¿æŒ: {results['semantic_before']['projection_similarity']:.4f} â†’ {results['semantic_after']['projection_similarity']:.4f}\n")
    
    print(f"ğŸ“„ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ° collaboration_report.txt")

if __name__ == "__main__":
    results = quick_test()
    save_detailed_results(results)
