#!/usr/bin/env python3
"""
ä»»åŠ¡æ„ŸçŸ¥åä½œæµ‹è¯•è„šæœ¬ï¼šä¸“é—¨æµ‹è¯•ä¸åŒä»»åŠ¡ç±»å‹ä¸‹çš„åä½œæ•ˆæœ
"""

import torch
import torch.nn.functional as F
import numpy as np
from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel
from Multi import (
    MultiModelCollaborator, TaskType, TaskDetector, TaskAwareTrainer
)

def test_task_detection(test_texts):
    """æµ‹è¯•ä»»åŠ¡æ£€æµ‹åŠŸèƒ½"""
    print("\nğŸ¯ ä»»åŠ¡æ£€æµ‹æµ‹è¯•")
    print("=" * 50)
    
    detector = TaskDetector()
    detection_results = {}
    
    for text, expected_task in test_texts:
        detected_task = detector.detect_task(text)
        detection_results[text] = {
            'expected': expected_task,
            'detected': detected_task,
            'correct': detected_task == expected_task
        }
        
        status = "âœ…" if detected_task == expected_task else "âŒ"
        print(f"{status} '{text}'")
        print(f"    é¢„æœŸ: {expected_task.value}")
        print(f"    æ£€æµ‹: {detected_task.value}")
    
    accuracy = sum(1 for r in detection_results.values() if r['correct']) / len(detection_results)
    print(f"\nğŸ“Š æ£€æµ‹å‡†ç¡®ç‡: {accuracy:.2%}")
    
    return detection_results

def test_task_aware_collaboration(collaborator, text, task_type):
    """æµ‹è¯•ç‰¹å®šä»»åŠ¡ç±»å‹ä¸‹çš„åä½œæ•ˆæœ"""
    print(f"\nğŸ” ä»»åŠ¡æ„ŸçŸ¥åä½œæµ‹è¯•: {task_type.value}")
    print(f"æ–‡æœ¬: '{text}'")
    
    # è·å–åŸå§‹hidden states
    hidden1 = collaborator.get_hidden_states(text, 0)  # BERT
    hidden2 = collaborator.get_hidden_states(text, 1)  # GPT-2
    
    # é€šç”¨åä½œï¼ˆä¸æŒ‡å®šä»»åŠ¡ç±»å‹ï¼‰
    general_output = collaborator.collaborate(text, 0, 1, task_type=None)
    
    # ä»»åŠ¡æ„ŸçŸ¥åä½œ
    task_aware_output = collaborator.collaborate(text, 0, 1, task_type=task_type)
    
    # åˆ†æåä½œæ•ˆæœ
    general_hidden = general_output['adapted_hidden']
    task_aware_hidden = task_aware_output['adapted_hidden']
    
    # ä¸ºäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦å°†åŸå§‹BERT hidden statesæŠ•å½±åˆ°å…±äº«ç©ºé—´
    projected_states = collaborator.central_processor.semantic_projector([hidden1])
    projected_bert = projected_states[0]
    
    # è®¡ç®—ä¸æŠ•å½±åBERTçš„ç›¸ä¼¼æ€§
    bert_to_general_sim = F.cosine_similarity(
        projected_bert.mean(dim=1), general_hidden.mean(dim=1)
    ).item()
    
    bert_to_task_sim = F.cosine_similarity(
        projected_bert.mean(dim=1), task_aware_hidden.mean(dim=1)
    ).item()
    
    # è®¡ç®—é€‚é…åçš„ä¿¡æ¯å¯†åº¦å˜åŒ–
    general_info_density = torch.std(general_hidden).item()
    task_aware_info_density = torch.std(task_aware_hidden).item()
    
    print(f"  ğŸ“Š ç›¸ä¼¼æ€§åˆ†æ:")
    print(f"    æŠ•å½±BERT â†’ é€šç”¨é€‚é…: {bert_to_general_sim:.4f}")
    print(f"    æŠ•å½±BERT â†’ ä»»åŠ¡é€‚é…: {bert_to_task_sim:.4f}")
    print(f"  ğŸ“ˆ ä¿¡æ¯å¯†åº¦:")
    print(f"    é€šç”¨é€‚é…: {general_info_density:.4f}")
    print(f"    ä»»åŠ¡é€‚é…: {task_aware_info_density:.4f}")
    
    return {
        'task_type': task_type,
        'general_similarity': bert_to_general_sim,
        'task_aware_similarity': bert_to_task_sim,
        'general_info_density': general_info_density,
        'task_aware_info_density': task_aware_info_density,
        'adaptation_difference': abs(bert_to_task_sim - bert_to_general_sim)
    }

def test_task_specific_adaptation(collaborator, task_test_cases):
    """æµ‹è¯•ä¸åŒä»»åŠ¡ç±»å‹çš„é€‚é…æ•ˆæœ"""
    print("\nğŸ”§ ä»»åŠ¡ç‰¹å®šé€‚é…æµ‹è¯•")
    print("=" * 50)
    
    adaptation_results = []
    
    for text, task_type in task_test_cases:
        result = test_task_aware_collaboration(collaborator, text, task_type)
        adaptation_results.append(result)
    
    # åˆ†æä»»åŠ¡ç‰¹å®šçš„é€‚é…æ•ˆæœ
    print(f"\nğŸ“Š ä»»åŠ¡é€‚é…æ•ˆæœæ±‡æ€»:")
    task_performance = {}
    
    for result in adaptation_results:
        task = result['task_type'].value
        if task not in task_performance:
            task_performance[task] = []
        task_performance[task].append(result['adaptation_difference'])
    
    for task, differences in task_performance.items():
        avg_difference = np.mean(differences)
        print(f"  {task}: å¹³å‡é€‚é…å·®å¼‚ = {avg_difference:.4f}")
    
    return adaptation_results

def task_aware_generation_test(collaborator, text, task_type):
    """ä»»åŠ¡æ„ŸçŸ¥çš„æ–‡æœ¬ç”Ÿæˆæµ‹è¯•"""
    if not hasattr(collaborator, 'gpt2_generator'):
        collaborator.gpt2_generator = GPT2LMHeadModel.from_pretrained("gpt2")
    
    tokenizer = collaborator.tokenizers[1]  # GPT-2 tokenizer
    
    # é€šç”¨åä½œç”Ÿæˆ
    general_collab = collaborator.collaborate(text, 0, 1, task_type=None)
    
    # ä»»åŠ¡æ„ŸçŸ¥åä½œç”Ÿæˆ
    task_aware_collab = collaborator.collaborate(text, 0, 1, task_type=task_type)
    
    # æ¨¡æ‹Ÿä½¿ç”¨ä¸åŒåä½œç»“æœè¿›è¡Œç”Ÿæˆï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    inputs = tokenizer.encode_plus(text, return_tensors="pt", padding=True, truncation=True)
    
    # è®¾ç½®ä¸åŒçš„ç”Ÿæˆå‚æ•°åŸºäºä»»åŠ¡ç±»å‹
    generation_params = {
        TaskType.QUESTION_ANSWERING: {'temperature': 0.7, 'top_p': 0.9},
        TaskType.TEXT_GENERATION: {'temperature': 0.9, 'top_p': 0.95},
        TaskType.CONVERSATION: {'temperature': 0.8, 'top_p': 0.9},
        TaskType.SENTIMENT_ANALYSIS: {'temperature': 0.6, 'top_p': 0.8},
        TaskType.GENERAL: {'temperature': 0.8, 'top_p': 0.9}
    }
    
    params = generation_params.get(task_type, generation_params[TaskType.GENERAL])
    
    with torch.no_grad():
        general_output = collaborator.gpt2_generator.generate(
            inputs['input_ids'],
            max_length=len(inputs['input_ids'][0]) + 15,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        
        task_aware_output = collaborator.gpt2_generator.generate(
            inputs['input_ids'],
            max_length=len(inputs['input_ids'][0]) + 15,
            num_return_sequences=1,
            temperature=params['temperature'],
            top_p=params['top_p'],
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    general_text = tokenizer.decode(general_output[0], skip_special_tokens=True)
    task_aware_text = tokenizer.decode(task_aware_output[0], skip_special_tokens=True)
    
    return {
        'input': text,
        'task_type': task_type.value,
        'general_generation': general_text,
        'task_aware_generation': task_aware_text,
        'generation_params': params
    }

def task_aware_classification_test(collaborator, text, task_type):
    """ä»»åŠ¡æ„ŸçŸ¥çš„åˆ†ç±»æµ‹è¯•"""
    # é€šç”¨åä½œ
    general_collab = collaborator.collaborate(text, 0, 1, task_type=None)
    general_hidden = general_collab['adapted_hidden']
    
    # ä»»åŠ¡æ„ŸçŸ¥åä½œ
    task_aware_collab = collaborator.collaborate(text, 0, 1, task_type=task_type)
    task_aware_hidden = task_aware_collab['adapted_hidden']
    
    # åˆ›å»ºç®€å•åˆ†ç±»å™¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not hasattr(collaborator, 'task_classifiers'):
        collaborator.task_classifiers = {}
    
    if task_type not in collaborator.task_classifiers:
        hidden_size = task_aware_hidden.size(-1)
        if task_type == TaskType.SENTIMENT_ANALYSIS:
            num_classes = 3  # æ­£é¢ã€ä¸­æ€§ã€è´Ÿé¢
            class_labels = ['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢']
        elif task_type == TaskType.QUESTION_ANSWERING:
            num_classes = 2  # æ˜¯é—®é¢˜ã€ä¸æ˜¯é—®é¢˜
            class_labels = ['éé—®é¢˜', 'é—®é¢˜']
        else:
            num_classes = 2  # é€šç”¨äºŒåˆ†ç±»
            class_labels = ['ç±»åˆ«A', 'ç±»åˆ«B']
        
        collaborator.task_classifiers[task_type] = {
            'classifier': torch.nn.Linear(hidden_size, num_classes),
            'labels': class_labels
        }
    
    classifier_info = collaborator.task_classifiers[task_type]
    classifier = classifier_info['classifier']
    labels = classifier_info['labels']
    
    with torch.no_grad():
        # é€šç”¨åä½œçš„åˆ†ç±»ç»“æœ
        general_pooled = general_hidden.mean(dim=1)
        general_logits = classifier(general_pooled)
        general_probs = F.softmax(general_logits, dim=-1)
        general_pred = int(torch.argmax(general_probs, dim=-1).item())
        
        # ä»»åŠ¡æ„ŸçŸ¥åä½œçš„åˆ†ç±»ç»“æœ
        task_aware_pooled = task_aware_hidden.mean(dim=1)
        task_aware_logits = classifier(task_aware_pooled)
        task_aware_probs = F.softmax(task_aware_logits, dim=-1)
        task_aware_pred = int(torch.argmax(task_aware_probs, dim=-1).item())
    
    return {
        'text': text,
        'task_type': task_type.value,
        'general_prediction': {
            'label': labels[general_pred],
            'confidence': general_probs[0][general_pred].item(),
            'all_probs': {labels[i]: general_probs[0][i].item() for i in range(len(labels))}
        },
        'task_aware_prediction': {
            'label': labels[task_aware_pred],
            'confidence': task_aware_probs[0][task_aware_pred].item(),
            'all_probs': {labels[i]: task_aware_probs[0][i].item() for i in range(len(labels))}
        }
    }

def task_aware_quick_test():
    """ä»»åŠ¡æ„ŸçŸ¥çš„å¿«é€Ÿæµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ä»»åŠ¡æ„ŸçŸ¥å¤šæ¨¡å‹åä½œæµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model1 = AutoModel.from_pretrained("bert-base-uncased")
    model2 = AutoModel.from_pretrained("gpt2")
    collaborator = MultiModelCollaborator([model1, model2])
    
    # å®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼š(æ–‡æœ¬, é¢„æœŸä»»åŠ¡ç±»å‹)
    task_test_cases = [
        ("What is the capital of France?", TaskType.QUESTION_ANSWERING),
        ("I love this movie, it's amazing!", TaskType.SENTIMENT_ANALYSIS),
        ("Generate a story about dragons.", TaskType.TEXT_GENERATION),
        ("Hello, how are you today?", TaskType.CONVERSATION),
        ("The weather is nice today.", TaskType.GENERAL),
        ("How does machine learning work?", TaskType.QUESTION_ANSWERING),
        ("This book is terrible.", TaskType.SENTIMENT_ANALYSIS),
        ("Write a poem about love.", TaskType.TEXT_GENERATION),
        ("Good morning everyone!", TaskType.CONVERSATION)
    ]
    
    # ç¬¬ä¸€é˜¶æ®µï¼šä»»åŠ¡æ£€æµ‹æµ‹è¯•
    print("\nğŸ“‹ ç¬¬ä¸€é˜¶æ®µï¼šä»»åŠ¡æ£€æµ‹èƒ½åŠ›æµ‹è¯•")
    detection_results = test_task_detection(task_test_cases)
    
    # ç¬¬äºŒé˜¶æ®µï¼šä»»åŠ¡æ„ŸçŸ¥åä½œæµ‹è¯•ï¼ˆè®­ç»ƒå‰ï¼‰
    print("\nğŸ” ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒå‰ä»»åŠ¡æ„ŸçŸ¥åä½œæµ‹è¯•")
    before_adaptation_results = test_task_specific_adaptation(collaborator, task_test_cases[:5])
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šæ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ„ŸçŸ¥æµ‹è¯•
    print("\nğŸ“ ç¬¬ä¸‰é˜¶æ®µï¼šä»»åŠ¡æ„ŸçŸ¥æ–‡æœ¬ç”Ÿæˆæµ‹è¯•")
    generation_test_cases = [
        ("The weather is", TaskType.GENERAL),
        ("What is the answer to", TaskType.QUESTION_ANSWERING),
        ("I feel", TaskType.SENTIMENT_ANALYSIS),
        ("Once upon a time", TaskType.TEXT_GENERATION)
    ]
    
    generation_results = []
    for text, task_type in generation_test_cases:
        print(f"\næµ‹è¯•: '{text}' (ä»»åŠ¡: {task_type.value})")
        result = task_aware_generation_test(collaborator, text, task_type)
        generation_results.append(result)
        print(f"  é€šç”¨ç”Ÿæˆ: {result['general_generation']}")
        print(f"  ä»»åŠ¡æ„ŸçŸ¥ç”Ÿæˆ: {result['task_aware_generation']}")
    
    # ç¬¬å››é˜¶æ®µï¼šåˆ†ç±»ä»»åŠ¡æ„ŸçŸ¥æµ‹è¯•
    print("\nğŸ·ï¸ ç¬¬å››é˜¶æ®µï¼šä»»åŠ¡æ„ŸçŸ¥åˆ†ç±»æµ‹è¯•")
    classification_test_cases = [
        ("I love programming!", TaskType.SENTIMENT_ANALYSIS),
        ("This is a terrible movie", TaskType.SENTIMENT_ANALYSIS),
        ("How do you solve this problem?", TaskType.QUESTION_ANSWERING),
        ("Tell me about the weather", TaskType.QUESTION_ANSWERING)
    ]
    
    classification_results = []
    for text, task_type in classification_test_cases:
        print(f"\næµ‹è¯•: '{text}' (ä»»åŠ¡: {task_type.value})")
        result = task_aware_classification_test(collaborator, text, task_type)
        classification_results.append(result)
        print(f"  é€šç”¨åˆ†ç±»: {result['general_prediction']['label']} "
              f"(ç½®ä¿¡åº¦: {result['general_prediction']['confidence']:.3f})")
        print(f"  ä»»åŠ¡æ„ŸçŸ¥åˆ†ç±»: {result['task_aware_prediction']['label']} "
              f"(ç½®ä¿¡åº¦: {result['task_aware_prediction']['confidence']:.3f})")
    
    # ç¬¬äº”é˜¶æ®µï¼šä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒ
    print("\nğŸ”§ ç¬¬äº”é˜¶æ®µï¼šä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒ")
    trainer = TaskAwareTrainer(collaborator, learning_rate=1e-4)
    
    # å‡†å¤‡å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®
    train_texts = [text for text, _ in task_test_cases] + [
        "Machine learning is powerful",
        "The ocean is vast and deep",
        "Music brings joy to people",
        "How does AI work?",
        "I'm feeling great today!",
        "Continue this story: A brave knight...",
        "Good evening everyone"
    ]
    
    print("å¼€å§‹ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒ...")
    trainer.train_with_task_awareness(train_texts, epochs=3)
    
    # ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒåæ•ˆæœå¯¹æ¯”
    print("\nğŸ“Š ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒåæ•ˆæœå¯¹æ¯”")
    after_adaptation_results = test_task_specific_adaptation(collaborator, task_test_cases[:5])
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\nğŸ“‹ ç¬¬ä¸ƒé˜¶æ®µï¼šä»»åŠ¡æ„ŸçŸ¥æ•ˆæœæ€»ç»“")
    print("=" * 60)
    
    # æ£€æµ‹å‡†ç¡®ç‡
    detection_accuracy = sum(1 for r in detection_results.values() if r['correct']) / len(detection_results)
    print(f"ğŸ¯ ä»»åŠ¡æ£€æµ‹å‡†ç¡®ç‡: {detection_accuracy:.2%}")
    
    # é€‚é…æ•ˆæœå¯¹æ¯”
    print(f"\nğŸ”§ ä»»åŠ¡æ„ŸçŸ¥é€‚é…æ•ˆæœ:")
    task_improvements = {}
    
    for i, (before, after) in enumerate(zip(before_adaptation_results, after_adaptation_results)):
        task_type = before['task_type'].value
        improvement = after['adaptation_difference'] - before['adaptation_difference']
        
        if task_type not in task_improvements:
            task_improvements[task_type] = []
        task_improvements[task_type].append(improvement)
        
        print(f"  {task_type}: é€‚é…å·®å¼‚ {before['adaptation_difference']:.4f} â†’ "
              f"{after['adaptation_difference']:.4f} (å˜åŒ–: {improvement:+.4f})")
    
    # æ•´ä½“è¯„ä¼°
    overall_improvement = np.mean([
        np.mean(improvements) for improvements in task_improvements.values()
    ])
    
    print(f"\nâœ¨ æ•´ä½“ä»»åŠ¡æ„ŸçŸ¥æ”¹å–„: {overall_improvement:+.4f}")
    
    if overall_improvement > 0.01:
        conclusion = "ğŸ‰ ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒæ˜¾è‘—æå‡äº†åä½œæ•ˆæœï¼"
    elif overall_improvement > 0.001:
        conclusion = "âœ… ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒæœ‰ä¸€å®šæ”¹å–„æ•ˆæœ"
    else:
        conclusion = "ğŸ“ˆ ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒæ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–è°ƒå‚"
    
    print(f"  {conclusion}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'detection_accuracy': detection_accuracy,
        'detection_results': detection_results,
        'before_adaptation': before_adaptation_results,
        'after_adaptation': after_adaptation_results,
        'generation_results': generation_results,
        'classification_results': classification_results,
        'task_improvements': task_improvements,
        'overall_improvement': overall_improvement,
        'conclusion': conclusion
    }
    
    return results

def save_task_aware_results(results):
    """ä¿å­˜ä»»åŠ¡æ„ŸçŸ¥æµ‹è¯•ç»“æœ"""
    import json
    
    # å‡†å¤‡ä¿å­˜æ•°æ®ï¼ˆå¤„ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
    save_data = {
        'summary': {
            'detection_accuracy': results['detection_accuracy'],
            'overall_improvement': results['overall_improvement'],
            'conclusion': results['conclusion']
        },
        'detection_details': {
            text: {
                'expected': result['expected'].value,
                'detected': result['detected'].value,
                'correct': result['correct']
            }
            for text, result in results['detection_results'].items()
        },
        'generation_examples': [
            {
                'input': r['input'],
                'task_type': r['task_type'],
                'general_generation': r['general_generation'],
                'task_aware_generation': r['task_aware_generation']
            }
            for r in results['generation_results']
        ],
        'classification_examples': [
            {
                'text': r['text'],
                'task_type': r['task_type'],
                'general_prediction': r['general_prediction'],
                'task_aware_prediction': r['task_aware_prediction']
            }
            for r in results['classification_results']
        ],
        'adaptation_improvements': {
            task: improvements for task, improvements in results['task_improvements'].items()
        }
    }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open('task_aware_results.json', 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    with open('task_aware_report.txt', 'w', encoding='utf-8') as f:
        f.write("ğŸ¤– ä»»åŠ¡æ„ŸçŸ¥å¤šæ¨¡å‹åä½œè¯¦ç»†æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡:\n")
        f.write(f"  ä»»åŠ¡æ£€æµ‹å‡†ç¡®ç‡: {results['detection_accuracy']:.2%}\n")
        f.write(f"  æ•´ä½“æ”¹å–„ç¨‹åº¦: {results['overall_improvement']:+.4f}\n")
        f.write(f"  ç»“è®º: {results['conclusion']}\n\n")
        
        f.write("ğŸ¯ ä»»åŠ¡æ£€æµ‹è¯¦æƒ…:\n")
        for text, result in results['detection_results'].items():
            status = "âœ…" if result['correct'] else "âŒ"
            f.write(f"{status} '{text}'\n")
            f.write(f"    é¢„æœŸ: {result['expected'].value} | æ£€æµ‹: {result['detected'].value}\n")
        
        f.write(f"\nğŸ“ ç”Ÿæˆä»»åŠ¡ç¤ºä¾‹:\n")
        for r in results['generation_results']:
            f.write(f"è¾“å…¥: '{r['input']}' (ä»»åŠ¡: {r['task_type']})\n")
            f.write(f"  é€šç”¨ç”Ÿæˆ: {r['general_generation']}\n")
            f.write(f"  ä»»åŠ¡æ„ŸçŸ¥ç”Ÿæˆ: {r['task_aware_generation']}\n\n")
        
        f.write(f"ğŸ·ï¸ åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹:\n")
        for r in results['classification_results']:
            f.write(f"æ–‡æœ¬: '{r['text']}' (ä»»åŠ¡: {r['task_type']})\n")
            f.write(f"  é€šç”¨åˆ†ç±»: {r['general_prediction']['label']} "
                   f"(ç½®ä¿¡åº¦: {r['general_prediction']['confidence']:.3f})\n")
            f.write(f"  ä»»åŠ¡æ„ŸçŸ¥åˆ†ç±»: {r['task_aware_prediction']['label']} "
                   f"(ç½®ä¿¡åº¦: {r['task_aware_prediction']['confidence']:.3f})\n\n")
    
    print(f"\nğŸ’¾ ä»»åŠ¡æ„ŸçŸ¥æµ‹è¯•ç»“æœå·²ä¿å­˜:")
    print(f"  ğŸ“„ è¯¦ç»†æ•°æ®: task_aware_results.json")
    print(f"  ğŸ“‹ æ–‡æœ¬æŠ¥å‘Š: task_aware_report.txt")

if __name__ == "__main__":
    """è¿è¡Œä»»åŠ¡æ„ŸçŸ¥æµ‹è¯•"""
    print("ğŸ¯ å¼€å§‹ä»»åŠ¡æ„ŸçŸ¥å¤šæ¨¡å‹åä½œæµ‹è¯•")
    print("=" * 60)
    
    try:
        results = task_aware_quick_test()
        save_task_aware_results(results)
        
        print("\nğŸ‰ ä»»åŠ¡æ„ŸçŸ¥æµ‹è¯•å®Œæˆï¼")
        print("æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶äº†è§£è¯¦ç»†ç»“æœã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹åŠ è½½å’Œä¾èµ–é¡¹æ˜¯å¦æ­£ç¡®å®‰è£…ã€‚")
        import traceback
        traceback.print_exc()
