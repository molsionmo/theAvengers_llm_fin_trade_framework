#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†CLIè„šæœ¬
ç”¨äºä¸‹è½½å’Œé¢„å¤„ç†å„ç§æ•°æ®é›†
"""

import argparse
import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.preprocessor import DataPreprocessor
from src.data.dataset_loader import DatasetLoader
from src.data.task_dataset import TaskAwareDataset


def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é¢„å¤„ç†å·¥å…·')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨çš„å‘½ä»¤')
    
    # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†')
    
    # ä¸‹è½½å•ä¸ªæ•°æ®é›†
    download_parser = subparsers.add_parser('download', help='ä¸‹è½½å•ä¸ªæ•°æ®é›†')
    download_parser.add_argument('dataset', help='æ•°æ®é›†åç§°')
    download_parser.add_argument('--split', default='train', help='æ•°æ®é›†åˆ†å‰² (train/validation/test)')
    download_parser.add_argument('--max-samples', type=int, help='æœ€å¤§æ ·æœ¬æ•°é‡')
    download_parser.add_argument('--output-dir', default='./processed_data', help='è¾“å‡ºç›®å½•')
    
    # åˆ›å»ºæ··åˆæ•°æ®é›†
    mixed_parser = subparsers.add_parser('create-mixed', help='åˆ›å»ºæ··åˆæ•°æ®é›†')
    mixed_parser.add_argument('datasets', nargs='+', help='æ•°æ®é›†åç§°åˆ—è¡¨')
    mixed_parser.add_argument('--max-samples-per-dataset', type=int, default=1000, 
                            help='æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°')
    mixed_parser.add_argument('--split', default='train', help='æ•°æ®é›†åˆ†å‰²')
    mixed_parser.add_argument('--output-dir', default='./processed_data', help='è¾“å‡ºç›®å½•')
    mixed_parser.add_argument('--cache-dir', default='./data_cache', help='æ•°æ®ç¼“å­˜ç›®å½•')
    
    # é¢„å¤„ç†ç”¨äºä»»åŠ¡æ£€æµ‹
    task_parser = subparsers.add_parser('preprocess-task', help='é¢„å¤„ç†æ•°æ®ç”¨äºä»»åŠ¡æ£€æµ‹')
    task_parser.add_argument('datasets', nargs='+', help='æ•°æ®é›†åç§°åˆ—è¡¨')
    task_parser.add_argument('--max-samples-per-dataset', type=int, default=500, 
                           help='æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°')
    task_parser.add_argument('--output-dir', default='./task_detection_data', help='è¾“å‡ºç›®å½•')
    
    # é¢„å¤„ç†ç”¨äºé€‚é…å™¨è®­ç»ƒ
    adapter_parser = subparsers.add_parser('preprocess-adapter', help='é¢„å¤„ç†æ•°æ®ç”¨äºé€‚é…å™¨è®­ç»ƒ')
    adapter_parser.add_argument('datasets', nargs='+', help='æ•°æ®é›†åç§°åˆ—è¡¨')
    adapter_parser.add_argument('--max-samples-per-dataset', type=int, default=1000, 
                              help='æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°')
    adapter_parser.add_argument('--tokenizer', default='bert-base-uncased', help='åˆ†è¯å™¨åç§°')
    adapter_parser.add_argument('--output-dir', default='./adapter_training_data', help='è¾“å‡ºç›®å½•')
    
    # éªŒè¯æ•°æ®é›†
    validate_parser = subparsers.add_parser('validate', help='éªŒè¯å¤„ç†åçš„æ•°æ®é›†')
    validate_parser.add_argument('data_dir', help='æ•°æ®ç›®å½•è·¯å¾„')
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats_parser = subparsers.add_parser('stats', help='æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯')
    stats_parser.add_argument('data_dir', help='æ•°æ®ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.command == 'list':
        list_datasets()
    elif args.command == 'download':
        download_dataset(args)
    elif args.command == 'create-mixed':
        create_mixed_dataset(args)
    elif args.command == 'preprocess-task':
        preprocess_for_task_detection(args)
    elif args.command == 'preprocess-adapter':
        preprocess_for_adapter_training(args)
    elif args.command == 'validate':
        validate_dataset(args)
    elif args.command == 'stats':
        show_statistics(args)
    else:
        parser.print_help()


def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    preprocessor = DataPreprocessor()
    datasets = preprocessor.list_available_datasets()
    
    print("å¯ç”¨æ•°æ®é›†:")
    print("=" * 60)
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
    task_groups = {}
    for name, config in datasets.items():
        task_type = config['task_type']
        if task_type not in task_groups:
            task_groups[task_type] = []
        task_groups[task_type].append(name)
    
    for task_type, dataset_names in task_groups.items():
        print(f"\nğŸ“‹ {task_type.replace('_', ' ').title()}:")
        for dataset_name in sorted(dataset_names):
            config = datasets[dataset_name]
            print(f"  â€¢ {dataset_name}")
            print(f"    HuggingFace: {config['huggingface_name']}")
            if 'subset' in config:
                print(f"    å­é›†: {config['subset']}")


def download_dataset(args):
    """ä¸‹è½½å•ä¸ªæ•°æ®é›†"""
    print(f"ä¸‹è½½æ•°æ®é›†: {args.dataset}")
    
    preprocessor = DataPreprocessor()
    
    try:
        # ä»»åŠ¡æ£€æµ‹æ•°æ®
        task_data = preprocessor.preprocess_for_task_detection(
            args.dataset, args.split, args.max_samples
        )
        
        # é€‚é…å™¨è®­ç»ƒæ•°æ®
        adapter_data = preprocessor.preprocess_for_adapter_training(
            args.dataset, args.split, args.max_samples
        )
        
        # ä¿å­˜æ•°æ®
        output_dir = Path(args.output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜ä»»åŠ¡æ£€æµ‹æ•°æ®
        task_file = output_dir / f"{args.dataset}_task_detection.json"
        with open(task_file, 'w', encoding='utf-8') as f:
            json.dump(task_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é€‚é…å™¨è®­ç»ƒæ•°æ®
        adapter_file = output_dir / f"{args.dataset}_adapter_training.json"
        with open(adapter_file, 'w', encoding='utf-8') as f:
            json.dump(adapter_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®é›† {args.dataset} ä¸‹è½½å®Œæˆ")
        print(f"   ä»»åŠ¡æ£€æµ‹æ•°æ®: {len(task_data)} æ¡è®°å½• -> {task_file}")
        print(f"   é€‚é…å™¨è®­ç»ƒæ•°æ®: {len(adapter_data['input_texts'])} æ¡è®°å½• -> {adapter_file}")
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")


def create_mixed_dataset(args):
    """åˆ›å»ºæ··åˆæ•°æ®é›†"""
    print(f"åˆ›å»ºæ··åˆæ•°æ®é›†: {args.datasets}")
    
    preprocessor = DataPreprocessor(cache_dir=args.cache_dir)
    
    try:
        mixed_data = preprocessor.create_mixed_dataset(
            args.datasets,
            max_samples_per_dataset=args.max_samples_per_dataset,
            split=args.split
        )
        
        # ä¿å­˜æ•°æ®
        preprocessor.save_processed_data(mixed_data, args.output_dir)
        
        print(f"âœ… æ··åˆæ•°æ®é›†åˆ›å»ºå®Œæˆ")
        print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"   ä»»åŠ¡æ£€æµ‹æ•°æ®: {len(mixed_data['task_detection_data'])} æ¡è®°å½•")
        print(f"   é€‚é…å™¨è®­ç»ƒæ•°æ®: {len(mixed_data['adapter_training_data']['input_texts'])} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {e}")


def preprocess_for_task_detection(args):
    """é¢„å¤„ç†æ•°æ®ç”¨äºä»»åŠ¡æ£€æµ‹"""
    print(f"é¢„å¤„ç†ä»»åŠ¡æ£€æµ‹æ•°æ®: {args.datasets}")
    
    preprocessor = DataPreprocessor()
    all_task_data = []
    
    for dataset_name in args.datasets:
        try:
            task_data = preprocessor.preprocess_for_task_detection(
                dataset_name, 'train', args.max_samples_per_dataset
            )
            all_task_data.extend(task_data)
            print(f"  âœ… {dataset_name}: {len(task_data)} æ¡è®°å½•")
            
        except Exception as e:
            print(f"  âŒ {dataset_name}: {e}")
    
    # ä¿å­˜æ•°æ®
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    output_file = output_dir / 'task_detection_data.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_task_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ä»»åŠ¡æ£€æµ‹æ•°æ®é¢„å¤„ç†å®Œæˆ")
    print(f"   æ€»è®¡: {len(all_task_data)} æ¡è®°å½•")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")


def preprocess_for_adapter_training(args):
    """é¢„å¤„ç†æ•°æ®ç”¨äºé€‚é…å™¨è®­ç»ƒ"""
    print(f"é¢„å¤„ç†é€‚é…å™¨è®­ç»ƒæ•°æ®: {args.datasets}")
    
    preprocessor = DataPreprocessor()
    all_adapter_data = {
        'input_texts': [],
        'target_texts': [],
        'labels': [],
        'task_types': [],
        'dataset_sources': []
    }
    
    for dataset_name in args.datasets:
        try:
            adapter_data = preprocessor.preprocess_for_adapter_training(
                dataset_name, 'train', args.max_samples_per_dataset, args.tokenizer
            )
            
            all_adapter_data['input_texts'].extend(adapter_data['input_texts'])
            all_adapter_data['target_texts'].extend(adapter_data['target_texts'])
            all_adapter_data['labels'].extend(adapter_data['labels'])
            all_adapter_data['task_types'].extend([adapter_data['task_type']] * len(adapter_data['input_texts']))
            all_adapter_data['dataset_sources'].extend([adapter_data['dataset_source']] * len(adapter_data['input_texts']))
            
            print(f"  âœ… {dataset_name}: {len(adapter_data['input_texts'])} æ¡è®°å½•")
            
        except Exception as e:
            print(f"  âŒ {dataset_name}: {e}")
    
    # ä¿å­˜æ•°æ®
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    output_file = output_dir / 'adapter_training_data.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_adapter_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… é€‚é…å™¨è®­ç»ƒæ•°æ®é¢„å¤„ç†å®Œæˆ")
    print(f"   æ€»è®¡: {len(all_adapter_data['input_texts'])} æ¡è®°å½•")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")


def validate_dataset(args):
    """éªŒè¯å¤„ç†åçš„æ•°æ®é›†"""
    print(f"éªŒè¯æ•°æ®é›†: {args.data_dir}")
    
    try:
        loader = DatasetLoader(args.data_dir)
        
        # æ£€æŸ¥ä»»åŠ¡æ£€æµ‹æ•°æ®
        task_dataset = loader.get_task_detection_dataset()
        print(f"âœ… ä»»åŠ¡æ£€æµ‹æ•°æ®é›†: {len(task_dataset)} æ¡è®°å½•")
        
        # æ£€æŸ¥é€‚é…å™¨è®­ç»ƒæ•°æ®
        adapter_dataset = loader.get_adapter_training_dataset()
        print(f"âœ… é€‚é…å™¨è®­ç»ƒæ•°æ®é›†: {len(adapter_dataset)} æ¡è®°å½•")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        task_dataloader = loader.get_dataloader(task_dataset, batch_size=2)
        adapter_dataloader = loader.get_dataloader(adapter_dataset, batch_size=2)
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        for batch in task_dataloader:
            print(f"âœ… ä»»åŠ¡æ£€æµ‹æ‰¹æ¬¡å½¢çŠ¶: {batch['input_ids'].shape}")
            break
        
        for batch in adapter_dataloader:
            print(f"âœ… é€‚é…å™¨è®­ç»ƒæ‰¹æ¬¡å½¢çŠ¶: {batch['input_ids'].shape}")
            break
        
        print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")


def show_statistics(args):
    """æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯: {args.data_dir}")
    
    try:
        loader = DatasetLoader(args.data_dir)
        stats = loader.get_data_statistics()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡")
        print("=" * 60)
        
        if 'task_detection' in stats:
            print("\nğŸ” ä»»åŠ¡æ£€æµ‹æ•°æ®:")
            td_stats = stats['task_detection']
            print(f"   æ€»æ ·æœ¬æ•°: {td_stats['total_samples']}")
            print(f"   ä»»åŠ¡åˆ†å¸ƒ:")
            for task, count in td_stats['task_distribution'].items():
                percentage = (count / td_stats['total_samples']) * 100
                print(f"     â€¢ {task}: {count} ({percentage:.1f}%)")
        
        if 'adapter_training' in stats:
            print("\nğŸ”§ é€‚é…å™¨è®­ç»ƒæ•°æ®:")
            at_stats = stats['adapter_training']
            print(f"   æ€»æ ·æœ¬æ•°: {at_stats['total_samples']}")
            print(f"   ä»»åŠ¡åˆ†å¸ƒ:")
            for task, count in at_stats['task_distribution'].items():
                percentage = (count / at_stats['total_samples']) * 100
                print(f"     â€¢ {task}: {count} ({percentage:.1f}%)")
            print(f"   æ•°æ®é›†åˆ†å¸ƒ:")
            for dataset, count in at_stats['dataset_distribution'].items():
                percentage = (count / at_stats['total_samples']) * 100
                print(f"     â€¢ {dataset}: {count} ({percentage:.1f}%)")
        
    except Exception as e:
        print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
