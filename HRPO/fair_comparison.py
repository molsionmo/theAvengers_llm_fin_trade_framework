#!/usr/bin/env python3
"""
å…¬å¹³å¯¹æ¯”ï¼šåŸºç¡€æ¨¡å‹ vs è®­ç»ƒæ¨¡å‹
ç›¸åŒæ•°æ®é›†ï¼Œç›¸åŒè¯„ä¼°æ¡ä»¶ä¸‹çš„æ€§èƒ½å¯¹æ¯”
"""

import json

def compare_models():
    print("=" * 80)
    print("ğŸ“Š å…¬å¹³å¯¹æ¯”ï¼šåŸºç¡€æ¨¡å‹ vs è®­ç»ƒæ¨¡å‹ (ç›¸åŒ200æ ·æœ¬)")
    print("=" * 80)
    
    # è¯»å–åŸºç¡€æ¨¡å‹ç»“æœ
    with open("./eval_results_base_model_200samples.json", 'r') as f:
        base_results = json.load(f)
    
    # è¯»å–è®­ç»ƒæ¨¡å‹ç»“æœ 
    with open("./experiments/Qwen2.5-1.5B-Instruct-gsm8k-group4-lora32-rmin0.98-temp0.5/checkpoint-2000/eval_results.json", 'r') as f:
        trained_results = json.load(f)
    
    base_metrics = base_results['metrics']
    trained_metrics = trained_results['metrics']
    
    print("\nğŸ“ˆ æ•´ä½“è¡¨ç°å¯¹æ¯”:")
    print(f"{'æ¨¡å‹ç±»å‹':<15} {'å‡†ç¡®ç‡':<10} {'æ­£ç¡®æ•°/æ€»æ•°':<12} {'ç™¾åˆ†æ¯”':<8}")
    print("-" * 50)
    print(f"{'åŸºç¡€æ¨¡å‹':<15} {base_metrics['accuracy']:<10.3f} {base_metrics['correct']}/{base_metrics['total']:<12} {base_metrics['accuracy']*100:.1f}%")
    print(f"{'è®­ç»ƒæ¨¡å‹':<15} {trained_metrics['accuracy']:<10.3f} {trained_metrics['correct']}/{trained_metrics['total']:<12} {trained_metrics['accuracy']*100:.1f}%")
    
    # è®¡ç®—æ”¹è¿›
    improvement = trained_metrics['accuracy'] - base_metrics['accuracy']
    relative_improvement = (improvement / base_metrics['accuracy']) * 100 if base_metrics['accuracy'] > 0 else 0
    
    print(f"\nğŸš€ æ€§èƒ½æå‡:")
    print(f"   ç»å¯¹æå‡: {improvement:+.3f} ({improvement*100:+.1f} ç™¾åˆ†ç‚¹)")
    print(f"   ç›¸å¯¹æå‡: {relative_improvement:+.1f}%")
    
    # åˆ†æå„æƒ…æ„Ÿç±»åˆ«çš„è¡¨ç°
    def analyze_sentiment_performance(results, model_name):
        sentiment_stats = {'positive': {'correct': 0, 'total': 0}, 
                          'negative': {'correct': 0, 'total': 0}, 
                          'neutral': {'correct': 0, 'total': 0}}
        
        for result in results['results']:
            true_sentiment = result['true_answer']
            if true_sentiment in sentiment_stats:
                sentiment_stats[true_sentiment]['total'] += 1
                if result['correct']:
                    sentiment_stats[true_sentiment]['correct'] += 1
        
        print(f"\nğŸ“Š {model_name}å„æƒ…æ„Ÿç±»åˆ«è¡¨ç°:")
        for sentiment, stats in sentiment_stats.items():
            if stats['total'] > 0:
                acc = stats['correct'] / stats['total']
                print(f"   {sentiment.upper():>8}: {stats['correct']:>2}/{stats['total']:<3} ({acc*100:>5.1f}%)")
        
        return sentiment_stats
    
    base_stats = analyze_sentiment_performance(base_results, "åŸºç¡€æ¨¡å‹")
    trained_stats = analyze_sentiment_performance(trained_results, "è®­ç»ƒæ¨¡å‹")
    
    # è¾“å‡ºè´¨é‡å¯¹æ¯”
    print(f"\nğŸ’¡ è¾“å‡ºè´¨é‡åˆ†æ:")
    
    # åˆ†æåŸºç¡€æ¨¡å‹è¾“å‡º
    base_sample_responses = [r['full_response'] for r in base_results['results'][:5]]
    base_avg_length = sum(len(r['full_response']) for r in base_results['results']) / len(base_results['results'])
    
    # åˆ†æè®­ç»ƒæ¨¡å‹è¾“å‡º
    trained_sample_responses = [r['full_response'] for r in trained_results['results'][:5]]
    trained_avg_length = sum(len(r['full_response']) for r in trained_results['results']) / len(trained_results['results'])
    
    print(f"   åŸºç¡€æ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {base_avg_length:.0f} å­—ç¬¦")
    print(f"   è®­ç»ƒæ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {trained_avg_length:.0f} å­—ç¬¦")
    
    print(f"\nğŸ“ è¾“å‡ºç¤ºä¾‹å¯¹æ¯”:")
    print(f"\nåŸºç¡€æ¨¡å‹å…¸å‹è¾“å‡º:")
    print(f"   '{base_sample_responses[0][:100]}{'...' if len(base_sample_responses[0]) > 100 else ''}'")
    
    print(f"\nè®­ç»ƒæ¨¡å‹å…¸å‹è¾“å‡º:")
    print(f"   '{trained_sample_responses[0][:100]}{'...' if len(trained_sample_responses[0]) > 100 else ''}'")
    
    # å„ç±»æƒ…æ„Ÿçš„æ”¹è¿›æƒ…å†µ
    print(f"\nğŸ¯ å„æƒ…æ„Ÿç±»åˆ«æ”¹è¿›æƒ…å†µ:")
    for sentiment in ['positive', 'negative', 'neutral']:
        if base_stats[sentiment]['total'] > 0 and trained_stats[sentiment]['total'] > 0:
            base_acc = base_stats[sentiment]['correct'] / base_stats[sentiment]['total']
            trained_acc = trained_stats[sentiment]['correct'] / trained_stats[sentiment]['total']
            improvement = trained_acc - base_acc
            print(f"   {sentiment.upper():>8}: {base_acc*100:>5.1f}% â†’ {trained_acc*100:>5.1f}% ({improvement*100:+.1f}%)")
    
    print(f"\nğŸ† æ€»ç»“:")
    if improvement > 0:
        print(f"   âœ… è®­ç»ƒæˆåŠŸï¼æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡")
        print(f"   ğŸ“ˆ å‡†ç¡®ç‡ä» {base_metrics['accuracy']*100:.1f}% æå‡åˆ° {trained_metrics['accuracy']*100:.1f}%")
        print(f"   ğŸ§  è¾“å‡ºè´¨é‡å¤§å¹…æ”¹å–„ï¼Œä»ç®€å•å›ç­”åˆ°å®Œæ•´æ¨ç†")
        print(f"   ğŸ¯ ç›¸å¯¹æ€§èƒ½æå‡ {relative_improvement:.1f}%")
    else:
        print(f"   âš ï¸  è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡ç•¥ä½ï¼Œä½†è¾“å‡ºè´¨é‡æœ‰æ˜¾è‘—æ”¹å–„")
    
    print(f"\nğŸ’ å…³é”®å‘ç°:")
    print(f"   1. è®­ç»ƒæ¨¡å‹å­¦ä¼šäº†ç»“æ„åŒ–æ¨ç†")
    print(f"   2. è¾“å‡ºæ ¼å¼æ›´åŠ è§„èŒƒå’Œè¯¦ç»†") 
    print(f"   3. è§£å†³äº†åŸºç¡€æ¨¡å‹å¯èƒ½çš„ç”Ÿæˆé—®é¢˜")
    print(f"   4. åœ¨ç›¸åŒè¯„ä¼°æ¡ä»¶ä¸‹å–å¾—äº†æ›´å¥½çš„æ€§èƒ½")
    
    print("\n" + "=" * 80)

if __name__ == "__main__":
    compare_models()
